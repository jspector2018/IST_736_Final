#Import Librariesfrom sklearn.feature_extraction.text import CountVectorizerimport pandas as pdfrom urllib.request import urlopen # instead of urllib2 like in Python 2.7import jsonimport numpy as npimport nltkimport stringimport refrom sklearn import naive_bayesfrom sklearn.naive_bayes import BernoulliNBfrom sklearn.naive_bayes import GaussianNBfrom sklearn.naive_bayes import MultinomialNBfrom sklearn import metricsfrom sklearn.metrics import accuracy_scorefrom nltk.corpus import stopwordsfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics import roc_auc_scoreimport numpy as np#multi-class classificationfrom sklearn.datasets import make_classificationfrom sklearn.linear_model import LogisticRegressionfrom sklearn.datasets import make_classificationfrom sklearn.linear_model import LogisticRegressionfrom sklearn.multiclass import OneVsRestClassifier#Loading Get It Done San Diego Datagid_csv = pd.read_csv('get_it_done_2020_ALL.csv') #static data of all requests in 2020 to date (11/25/20)gid_json = "http://san-diego.spotreporters.com/open311/v2/requests.json" #live API connection (most recent 50 requests)with urlopen("http://san-diego.spotreporters.com/open311/v2/requests.json") as response:    source = response.read()#print(source)gid = json.loads(source)print(json.dumps(gid, indent = 2)) #Used to better read the json data#save json url to filewith open('gid.json', 'w') as f:    json.dump(gid, f)#Store json data in a data framedf = pd.read_json('gid.json', orient = 'columns')#Drop unwanted columnsdf_cln = df.drop(columns=['service_code', 'status_notes', 'media_url'])pd.value_counts(df_cln['service_name'])'''Illegal Dumping      31Encampment           14Parking Issue         3Missed Collection     2'''#Filtering data with multiple criteriagid_cln = gid_csv[(gid_csv['service_name'] == 'Missed Collection') | (gid_csv['service_name'] == 'Illegal Dumping') |(gid_csv['service_name'] == 'Graffiti Removal') | (gid_csv['service_name'] == 'Encampment') | (gid_csv['service_name'] == '72 Hour Violation') |(gid_csv['service_name'] == 'Pothole') | (gid_csv['service_name'] == 'Other') | (gid_csv['service_name'] == 'Shared Mobility Device') |(gid_csv['service_name'] == 'Street Light Out') | (gid_csv['service_name'] == 'Parking Zone Violation') | (gid_csv['service_name'] == 'Sidewalk Repair Issue') |(gid_csv['service_name'] == 'Traffic Sign - Maintain') | (gid_csv['service_name'] == 'Dead Animal') |(gid_csv['service_name'] == 'Tree/Limb Fallen/Hanging')]#Specify Relevant Columns to Keepkeep = ['service_name','public_description']gid_clnR = gid_cln[keep]len(gid_clnR)#203702 requests'''Multinomial Naive Bayes    tfidVectorizder'''#Creating Training and Testing Datasets 70/30mnb = gid_clnRprint(len(mnb))#print(len(mnb2))#58250mnb_cln = mnb.dropna()print(mnb_cln)#print(mnb_cln2)#[51350 rows x 2 columns]mnb_cln.head()stopset = set(stopwords.words('english'))vectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words = stopset)y = mnb_cln.service_namex = vectorizer.fit_transform(mnb_cln.public_description)  print(y.shape)print(x.shape)#(51350,)#(51350, 18314)#There are 51350 observations and 18314 unique words#set test train splitx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 36)#train naive bayes classifierclf = naive_bayes.MultinomialNB()clf.fit(x_train, y_train)#test model accuracyroc_auc_score(y_test, clf.predict_proba(x_test)[:,1], multi_class = 'ovr')#0.9822892491459847#Test with user originated textcase311_array=np.array(["I have some guys living in a tent here on my property. This is nuts..."])case311_vector = vectorizer.transform(case311_array)print(clf.predict(case311_vector))#['Encampment']'''The result of this test is accurate. The intention of user input is that thereis an encampment on their property and they want the city to come take care of the situation.'''#Creating a similar program but now it requests input from the userrequest_input = input("What your non-emergency issue / request? ")#Test with user originated textcase311_array=np.array([request_input])case311_vector = vectorizer.transform(case311_array)print(clf.predict(case311_vector))'''Test results of user inputsWhat is the issue? there is a homeless person['Encampment']What is the issue? Someone dumped trash in the street['Illegal Dumping']''''''Multinomial Naive Bayes    Count Vectorization'''#stopwordsstopwords = nltk.corpus.stopwords.words('english')ps = nltk.PorterStemmer()cv = CountVectorizer()mnb_cv = gid_csv#Filtering data with multiple criteriamnb_cv_cln = mnb_cv[(mnb_cv['service_name'] == 'Illegal Dumping') | (mnb_cv['service_name'] == 'Encampment')]keep = ['service_name','public_description']mnb_cv_clnR = mnb_cv_cln[keep]print(len(mnb_cv_clnR))#58250mnb_cv_clnRR = mnb_cv_clnR.dropna()print(mnb_cv_clnRR)#[51350 rows x 2 columns]#Learningx_cv = cv.fit(mnb_cv_clnRR)print(x_cv.vocabulary_)print(x_cv.get_feature_names())#Create document term matrix x_cv = cv.transform(mnb_cv_clnRR) print(x_cv.shape)print(x_cv)print(x_cv.toarray())#function to clean the textdef clean_txt(txt):    txt = "".join([c for c in txt if c not in string.punctuation])    tokens = re.split('\W+', txt)    txt = [ps.stem(word) for word in tokens if word not in stopwords]    return txtcv1 = CountVectorizer(analyzer = clean_txt)x1 = cv.fit_transform(mnb_cv_clnRR["public_description"])print(x1.shape)#(51350, 18463)print(cv1.get_feature_names())'''BernoulliFor Bernoulli, when using CountVectorizer , be sure to use the binary=True in the CountVectorizer definition.'''bernNB = BernoulliNB(binarize=True)bernNB.fit(x_train, y_train)print(bernNB)y_expect = y_testy_pred = bernNB.predict(x_test)print(accuracy_score(y_expect, y_pred))#0.6249415796853093bernNB = BernoulliNB(binarize=0.1)bernNB.fit(x_train, y_train)print(bernNB)y_expect = y_testy_pred = bernNB.predict(x_test)print(accuracy_score(y_expect, y_pred))#0.8918055771927091'''Adjusting the parameter settings made the model much better.''''''End of work.Notes Area...'''#Creating Training and Testing Datasets 70/30bern = gid_clnRRprint(len(bern))#58250bern_cln = bern.dropna()print(bern_cln)#[51350 rows x 2 columns]bern_cln.head()bern_trn = bern_cln.sample(frac=0.7, random_state=1)bern_tst = bern_cln.drop(bern_trn.index)#Checking workif len(bern_cln) == len(bern_trn) + len(bern_tst):    print("Your math is correct, congratulations...")bernNB = BernoulliNB(binarize=True)bernNB.fit(bern_trn, bern_tst)print(bernNB)print(len(bern))#58250'''Naive BayesFor Na√Øve Bayes, you should use CountVectorizer and also TfidfVectorizer so that you can compare them. '''#stopwordsstopwords = nltk.corpus.stopwords.words('english')ps = nltk.PorterStemmer()#function to clean the textdef clean_txt(txt):    txt = "".join([c for c in txt if c not in string.punctuation])    tokens = re.split('\W+', txt)    txt = [ps.stem(word) for word in tokens if word not in stopwords]    return txt#Count Vectorizercv = CountVectorizer()X = cv.fit(bern)print(X.vocabulary_)print(cv.get_feature_names())X = cv.transform(bern)print(X.shape)